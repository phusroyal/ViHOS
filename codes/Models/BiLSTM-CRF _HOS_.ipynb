{"cells":[{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Model should be trained multiple times with different random seeds to get the best model."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":41483,"status":"ok","timestamp":1641748295070,"user":{"displayName":"Nap Lan Dau","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgyLJ7xblBnvLEyFishN4Q4xZSVXTxeaLzYRJU=s64","userId":"01595996840706054717"},"user_tz":-420},"id":"lqCp3rbqNGnO","outputId":"6567c635-78f2-4f41-c753-973e1995d8a9"},"outputs":[{"name":"stdout","output_type":"stream","text":["Collecting tensorflow-gpu==1.15.0\n","  Downloading tensorflow_gpu-1.15.0-cp37-cp37m-manylinux2010_x86_64.whl (411.5 MB)\n","\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 411.5 MB 7.2 kB/s \n","\u001b[?25hCollecting keras-applications>=1.0.8\n","  Downloading Keras_Applications-1.0.8-py3-none-any.whl (50 kB)\n","\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 50 kB 8.9 MB/s \n","\u001b[?25hRequirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==1.15.0) (1.13.3)\n","Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==1.15.0) (1.42.0)\n","Requirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==1.15.0) (0.2.0)\n","Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==1.15.0) (3.3.0)\n","Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==1.15.0) (3.17.3)\n","Collecting tensorflow-estimator==1.15.1\n","  Downloading tensorflow_estimator-1.15.1-py2.py3-none-any.whl (503 kB)\n","\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 503 kB 96.1 MB/s \n","\u001b[?25hRequirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==1.15.0) (0.12.0)\n","Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==1.15.0) (0.37.0)\n","Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==1.15.0) (0.8.1)\n","Collecting tensorboard<1.16.0,>=1.15.0\n","  Downloading tensorboard-1.15.0-py3-none-any.whl (3.8 MB)\n","\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3.8 MB 70.2 MB/s \n","\u001b[?25hRequirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==1.15.0) (1.15.0)\n","Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==1.15.0) (1.1.0)\n","Collecting gast==0.2.2\n","  Downloading gast-0.2.2.tar.gz (10 kB)\n","Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==1.15.0) (1.1.2)\n","Requirement already satisfied: numpy<2.0,>=1.16.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==1.15.0) (1.19.5)\n","Requirement already satisfied: h5py in /usr/local/lib/python3.7/dist-packages (from keras-applications>=1.0.8->tensorflow-gpu==1.15.0) (3.1.0)\n","Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow-gpu==1.15.0) (57.4.0)\n","Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow-gpu==1.15.0) (1.0.1)\n","Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow-gpu==1.15.0) (3.3.6)\n","Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow-gpu==1.15.0) (4.8.2)\n","Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow-gpu==1.15.0) (3.10.0.2)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow-gpu==1.15.0) (3.6.0)\n","Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py->keras-applications>=1.0.8->tensorflow-gpu==1.15.0) (1.5.2)\n","Building wheels for collected packages: gast\n","  Building wheel for gast (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for gast: filename=gast-0.2.2-py3-none-any.whl size=7554 sha256=1b93f5427596dc4368671f3a698e630809f98ee3d4e6fd038d40466644292341\n","  Stored in directory: /root/.cache/pip/wheels/21/7f/02/420f32a803f7d0967b48dd823da3f558c5166991bfd204eef3\n","Successfully built gast\n","Installing collected packages: tensorflow-estimator, tensorboard, keras-applications, gast, tensorflow-gpu\n","  Attempting uninstall: tensorflow-estimator\n","    Found existing installation: tensorflow-estimator 2.7.0\n","    Uninstalling tensorflow-estimator-2.7.0:\n","      Successfully uninstalled tensorflow-estimator-2.7.0\n","  Attempting uninstall: tensorboard\n","    Found existing installation: tensorboard 2.7.0\n","    Uninstalling tensorboard-2.7.0:\n","      Successfully uninstalled tensorboard-2.7.0\n","  Attempting uninstall: gast\n","    Found existing installation: gast 0.4.0\n","    Uninstalling gast-0.4.0:\n","      Successfully uninstalled gast-0.4.0\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","tensorflow 2.7.0 requires tensorboard~=2.6, but you have tensorboard 1.15.0 which is incompatible.\n","tensorflow 2.7.0 requires tensorflow-estimator<2.8,~=2.7.0rc0, but you have tensorflow-estimator 1.15.1 which is incompatible.\n","tensorflow-probability 0.15.0 requires gast>=0.3.2, but you have gast 0.2.2 which is incompatible.\u001b[0m\n","Successfully installed gast-0.2.2 keras-applications-1.0.8 tensorboard-1.15.0 tensorflow-estimator-1.15.1 tensorflow-gpu-1.15.0\n"]}],"source":["pip install tensorflow-gpu==1.15.0"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3075,"status":"ok","timestamp":1641748298138,"user":{"displayName":"Nap Lan Dau","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgyLJ7xblBnvLEyFishN4Q4xZSVXTxeaLzYRJU=s64","userId":"01595996840706054717"},"user_tz":-420},"id":"7MfrOvXwNqdM","outputId":"21a63bf2-0f43-4464-9dd1-4e90a19f3902"},"outputs":[{"name":"stdout","output_type":"stream","text":["Collecting keras==2.2.4\n","  Downloading Keras-2.2.4-py2.py3-none-any.whl (312 kB)\n","\u001b[?25l\r\u001b[K     |‚ñà                               | 10 kB 37.5 MB/s eta 0:00:01\r\u001b[K     |‚ñà‚ñà                              | 20 kB 29.9 MB/s eta 0:00:01\r\u001b[K     |‚ñà‚ñà‚ñà‚ñè                            | 30 kB 19.8 MB/s eta 0:00:01\r\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñè                           | 40 kB 16.0 MB/s eta 0:00:01\r\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                          | 51 kB 8.4 MB/s eta 0:00:01\r\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                         | 61 kB 9.8 MB/s eta 0:00:01\r\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                        | 71 kB 9.0 MB/s eta 0:00:01\r\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                       | 81 kB 10.1 MB/s eta 0:00:01\r\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                      | 92 kB 10.2 MB/s eta 0:00:01\r\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                     | 102 kB 8.2 MB/s eta 0:00:01\r\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                    | 112 kB 8.2 MB/s eta 0:00:01\r\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                   | 122 kB 8.2 MB/s eta 0:00:01\r\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                  | 133 kB 8.2 MB/s eta 0:00:01\r\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                 | 143 kB 8.2 MB/s eta 0:00:01\r\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                | 153 kB 8.2 MB/s eta 0:00:01\r\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä               | 163 kB 8.2 MB/s eta 0:00:01\r\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ              | 174 kB 8.2 MB/s eta 0:00:01\r\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ             | 184 kB 8.2 MB/s eta 0:00:01\r\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà            | 194 kB 8.2 MB/s eta 0:00:01\r\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà           | 204 kB 8.2 MB/s eta 0:00:01\r\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà          | 215 kB 8.2 MB/s eta 0:00:01\r\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà         | 225 kB 8.2 MB/s eta 0:00:01\r\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè       | 235 kB 8.2 MB/s eta 0:00:01\r\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè      | 245 kB 8.2 MB/s eta 0:00:01\r\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè     | 256 kB 8.2 MB/s eta 0:00:01\r\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 266 kB 8.2 MB/s eta 0:00:01\r\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 276 kB 8.2 MB/s eta 0:00:01\r\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 286 kB 8.2 MB/s eta 0:00:01\r\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 296 kB 8.2 MB/s eta 0:00:01\r\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 307 kB 8.2 MB/s eta 0:00:01\r\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 312 kB 8.2 MB/s \n","\u001b[?25hRequirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from keras==2.2.4) (3.13)\n","Requirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python3.7/dist-packages (from keras==2.2.4) (1.0.8)\n","Requirement already satisfied: h5py in /usr/local/lib/python3.7/dist-packages (from keras==2.2.4) (3.1.0)\n","Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.7/dist-packages (from keras==2.2.4) (1.4.1)\n","Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.7/dist-packages (from keras==2.2.4) (1.1.2)\n","Requirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python3.7/dist-packages (from keras==2.2.4) (1.19.5)\n","Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.7/dist-packages (from keras==2.2.4) (1.15.0)\n","Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py->keras==2.2.4) (1.5.2)\n","Installing collected packages: keras\n","  Attempting uninstall: keras\n","    Found existing installation: keras 2.7.0\n","    Uninstalling keras-2.7.0:\n","      Successfully uninstalled keras-2.7.0\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","tensorflow 2.7.0 requires keras<2.8,>=2.7.0rc0, but you have keras 2.2.4 which is incompatible.\n","tensorflow 2.7.0 requires tensorboard~=2.6, but you have tensorboard 1.15.0 which is incompatible.\n","tensorflow 2.7.0 requires tensorflow-estimator<2.8,~=2.7.0rc0, but you have tensorflow-estimator 1.15.1 which is incompatible.\u001b[0m\n","Successfully installed keras-2.2.4\n"]}],"source":["!pip install keras==2.2.4"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4412,"status":"ok","timestamp":1641748302546,"user":{"displayName":"Nap Lan Dau","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgyLJ7xblBnvLEyFishN4Q4xZSVXTxeaLzYRJU=s64","userId":"01595996840706054717"},"user_tz":-420},"id":"-KutAPa4Nvs5","outputId":"c1f2249b-d562-47e4-aac6-6c962b34e5c4"},"outputs":[{"name":"stdout","output_type":"stream","text":["Collecting git+https://www.github.com/keras-team/keras-contrib.git\n","  Cloning https://www.github.com/keras-team/keras-contrib.git to /tmp/pip-req-build-xiba1f3l\n","  Running command git clone -q https://www.github.com/keras-team/keras-contrib.git /tmp/pip-req-build-xiba1f3l\n","Requirement already satisfied: keras in /usr/local/lib/python3.7/dist-packages (from keras-contrib==2.0.8) (2.2.4)\n","Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.7/dist-packages (from keras->keras-contrib==2.0.8) (1.1.2)\n","Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from keras->keras-contrib==2.0.8) (3.13)\n","Requirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python3.7/dist-packages (from keras->keras-contrib==2.0.8) (1.19.5)\n","Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.7/dist-packages (from keras->keras-contrib==2.0.8) (1.15.0)\n","Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.7/dist-packages (from keras->keras-contrib==2.0.8) (1.4.1)\n","Requirement already satisfied: h5py in /usr/local/lib/python3.7/dist-packages (from keras->keras-contrib==2.0.8) (3.1.0)\n","Requirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python3.7/dist-packages (from keras->keras-contrib==2.0.8) (1.0.8)\n","Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py->keras->keras-contrib==2.0.8) (1.5.2)\n","Building wheels for collected packages: keras-contrib\n","  Building wheel for keras-contrib (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for keras-contrib: filename=keras_contrib-2.0.8-py3-none-any.whl size=101077 sha256=918f218497b8f67389c6e9106991e779720ceeff165037703381a93d3a366ae4\n","  Stored in directory: /tmp/pip-ephem-wheel-cache-5rt_zec8/wheels/bb/1f/f2/b57495012683b6b20bbae94a3915ec79753111452d79886abc\n","Successfully built keras-contrib\n","Installing collected packages: keras-contrib\n","Successfully installed keras-contrib-2.0.8\n"]}],"source":["!pip install git+https://www.github.com/keras-team/keras-contrib.git"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":85610,"status":"ok","timestamp":1641748388148,"user":{"displayName":"Nap Lan Dau","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgyLJ7xblBnvLEyFishN4Q4xZSVXTxeaLzYRJU=s64","userId":"01595996840706054717"},"user_tz":-420},"id":"0df7fmkYTUOo","outputId":"46752bdf-200f-4304-e928-44cf4f713d39"},"outputs":[{"name":"stdout","output_type":"stream","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"markdown","metadata":{"id":"T5h8vjpOOA66"},"source":["# LOAD DATA"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2848,"status":"ok","timestamp":1641748390990,"user":{"displayName":"Nap Lan Dau","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgyLJ7xblBnvLEyFishN4Q4xZSVXTxeaLzYRJU=s64","userId":"01595996840706054717"},"user_tz":-420},"id":"hkXzai4kOCAW","outputId":"dc98305b-d417-43b8-f347-22e3a152c9b2"},"outputs":[{"name":"stderr","output_type":"stream","text":["Using TensorFlow backend.\n"]}],"source":["from tensorflow import keras \n","import pandas as pd\n","import numpy as np\n","from ast import literal_eval\n","from nltk.tokenize import TweetTokenizer\n","import spacy\n","from copy import deepcopy\n","from keras.preprocessing.sequence import pad_sequences\n","from tensorflow.keras.utils import to_categorical\n","from keras.initializers import Constant\n","from nltk.corpus import stopwords"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6,"status":"ok","timestamp":1641748390990,"user":{"displayName":"Nap Lan Dau","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgyLJ7xblBnvLEyFishN4Q4xZSVXTxeaLzYRJU=s64","userId":"01595996840706054717"},"user_tz":-420},"id":"1fx7ahrmUHml","outputId":"9594175c-40c5-419f-e246-d01e3e9a401f"},"outputs":[{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:8: FutureWarning: Passing a negative integer is deprecated in version 1.0 and will not be supported in future version. Instead, use None to not limit the column width.\n","  \n"]}],"source":["from IPython.core.interactiveshell import InteractiveShell\n","InteractiveShell.ast_node_interactivity = \"all\"\n","\n","\n","pd.set_option('display.max_rows', None)\n","pd.set_option('display.max_columns', None)\n","pd.set_option('display.width', None)\n","pd.set_option('display.max_colwidth', -1)\n","pd.options.display.max_rows"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EXqKe1GiN97b"},"outputs":[],"source":["\n","data = pd.read_csv('train.csv')\n","test = pd.read_csv('test.csv')\n","dev = pd.read_csv('dev.csv')\n","\n","text_data = data['content'].values\n","spans_data = data['index_spans'].apply(literal_eval)\n","lbl_data = [1 if len(s) > 0 else 0 for s in spans_data]\n","\n","text_test = test['content'].values\n","spans_test = test['index_spans'].apply(literal_eval)\n","lbl_test = [1 if len(s) > 0 else 0 for s in spans_test]\n","\n","text_dev = dev['content'].values\n","spans_dev = dev['index_spans'].apply(literal_eval)\n","lbl_dev = [1 if len(s) > 0 else 0 for s in spans_dev]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sQk8yXbaPHPI"},"outputs":[],"source":["tokenizer = TweetTokenizer()\n","\n","def tokenize_text(text):\n","  return tokenizer.tokenize(text)\n","\n","def toxic_word(span, text):\n","  i = 0\n","  token = []\n","  a = 0\n","  word = []\n","\n","  while (i < (len(span) - 1)):\n","      if (span[i] != (span[i+1]-1)):\n","          token.append(span[a:(i+1)])\n","          a = i + 1\n","      elif i == (len(span) - 2):\n","          token.append(span[a:i+2])\n","      i = i + 1\n","  for t in token:\n","      word.append(text[t[0]:(t[len(t)-1])+1])\n","  return word\n","def span_convert(text_data, spans):\n","    MAX_LEN = 0\n","    token_labels = []\n","\n","    for i in range(0, len(text_data)):\n","        token_labels.append(toxic_word(spans[i], text_data[i]))\n","\n","    lst_seq = []\n","    for i in range(0, len(text_data)):\n","        token = tokenize_text(text_data[i])\n","        if len(token) > MAX_LEN:\n","            MAX_LEN = len(token)\n","            \n","        seq = np.zeros(len(token), dtype=int)\n","        for j in range(0, len(token)):\n","            for t in token_labels[i]:\n","                if token[j] in tokenize_text(t):\n","                    seq[j] = 1\n","        lst_seq.append(seq)     \n","\n","    return (token_labels, lst_seq)\n","def span_retrived(text_data, spans):\n","    token_labels = []\n","\n","    for i in range(0, len(text_data)):\n","        token_labels.append(toxic_word(spans[i], text_data[i]))\n","    \n","    return token_labels\n","def decoding(text_data, encoding_text, prediction):\n","    test = [[idx2word[i] for i in row] for row in encoding_text]\n","\n","    lst_token = []\n","\n","    for t in range(0, len(test)):\n","        yy_pred = []\n","        for i in range(0, len(test[t])):\n","            if prediction[t][i] == 1:\n","                yy_pred.append(test[t][i])\n","        lst_token.append(yy_pred)\n","\n","    lis_idx = []\n","    for i in range(0, len(text_data)):\n","        idx = []\n","        for t in lst_token[i]:\n","            index = text_data[i].find(t)\n","            idx.append(index)\n","            for j in range(1, len(t)):\n","                index = index + 1\n","                idx.append(index)\n","        lis_idx.append(idx)\n","\n","    return lis_idx"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":546},"executionInfo":{"elapsed":26,"status":"ok","timestamp":1641748394534,"user":{"displayName":"Nap Lan Dau","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgyLJ7xblBnvLEyFishN4Q4xZSVXTxeaLzYRJU=s64","userId":"01595996840706054717"},"user_tz":-420},"id":"ZN7bRD9KQBkJ","outputId":"0b5ffb01-0e3f-43ad-ed1b-43b19da2819d"},"outputs":[{"data":{"text/html":["\n","  <div id=\"df-844d1c41-6202-41f4-bb58-6cc62a3ae3ce\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Unnamed: 0</th>\n","      <th>level_0</th>\n","      <th>index</th>\n","      <th>content</th>\n","      <th>index_spans</th>\n","      <th>token</th>\n","      <th>seq</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0</td>\n","      <td>2051</td>\n","      <td>104</td>\n","      <td>√Ä th·∫ø b·∫°n √Ω h√°t b√†i j jui c√≥ ai gi·ªõi thi·ªáu cho m√¨nh ƒëc ko. L√¢u jui m√¨nh ch·∫≥ng nghe b√†i h√°t m·ªõi n√†o c·∫£</td>\n","      <td>[]</td>\n","      <td>[]</td>\n","      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>1</td>\n","      <td>2087</td>\n","      <td>140</td>\n","      <td>Th·∫ßy ƒëang nghƒ© v·ªÅ c√¥ em xinh t∆∞∆°i n√†y ...</td>\n","      <td>[]</td>\n","      <td>[]</td>\n","      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>2</td>\n","      <td>2357</td>\n","      <td>410</td>\n","      <td>·ª¶a t·ª© qu√Ω c√≥ ch·∫∑t ƒë√¥i heo ƒëc ƒë√¢u, l·ªô t·ª© qu√Ω r·ªìi nhaaa hahhahaha t cho m th√∫i Tr·∫ßn Th·ªã Linh √Ånh Bui Thi My Ngoc Thanh H√† Thanh Th·∫£o Th√πy Trang L√™ Th·ªã Oanh Ho√†ng Linhc</td>\n","      <td>[]</td>\n","      <td>[]</td>\n","      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>3</td>\n","      <td>2588</td>\n","      <td>641</td>\n","      <td>t·ªët, c·ª© gi√° cao ƒëi -_- ch·ª© bh n√¥ng d√¢n c≈©ng mua ƒëc xe th√¨ k bi·∫øt ƒë∆∞·ªùng n√≥ c√≤n ra c√°i g√¨</td>\n","      <td>[]</td>\n","      <td>[]</td>\n","      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>4</td>\n","      <td>3854</td>\n","      <td>1907</td>\n","      <td>M·∫•y c√°i quan b·ªánh t·∫≠t c·ªßa th·ªùi ƒë·∫°i</td>\n","      <td>[]</td>\n","      <td>[]</td>\n","      <td>[0, 0, 0, 0, 0, 0, 0, 0]</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>5</td>\n","      <td>2565</td>\n","      <td>618</td>\n","      <td>M·∫∑t n√≥ l·∫°i ng√°o ü§£</td>\n","      <td>[]</td>\n","      <td>[]</td>\n","      <td>[0, 0, 0, 0, 0]</td>\n","    </tr>\n","    <tr>\n","      <th>6</th>\n","      <td>6</td>\n","      <td>617</td>\n","      <td>617</td>\n","      <td>Ngu cmm.</td>\n","      <td>[0, 1, 2, 3, 4, 5, 6]</td>\n","      <td>[Ngu cmm]</td>\n","      <td>[1, 1, 0]</td>\n","    </tr>\n","    <tr>\n","      <th>7</th>\n","      <td>7</td>\n","      <td>918</td>\n","      <td>918</td>\n","      <td>ai coi tr·∫ßn d·∫ßn v√† ng√¥ k·ª∑ l√† k·∫ª ngu ng√≥c...</td>\n","      <td>[29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39]</td>\n","      <td>[k·∫ª ngu ng√≥c]</td>\n","      <td>[0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0]</td>\n","    </tr>\n","    <tr>\n","      <th>8</th>\n","      <td>8</td>\n","      <td>2118</td>\n","      <td>171</td>\n","      <td>Nh∆∞ng m√† khi t·ªè t√¨nh xong r·ªìi b·∫•t b·∫°i v·ª´a m·∫•t ng m√¨nh th∆∞∆°ng v·ª´a m·∫•t lu√¥n b·∫°n üôÇ</td>\n","      <td>[]</td>\n","      <td>[]</td>\n","      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]</td>\n","    </tr>\n","    <tr>\n","      <th>9</th>\n","      <td>9</td>\n","      <td>3170</td>\n","      <td>1223</td>\n","      <td>t·ªôc caaaa</td>\n","      <td>[]</td>\n","      <td>[]</td>\n","      <td>[0, 0]</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-844d1c41-6202-41f4-bb58-6cc62a3ae3ce')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-844d1c41-6202-41f4-bb58-6cc62a3ae3ce button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-844d1c41-6202-41f4-bb58-6cc62a3ae3ce');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "],"text/plain":["   Unnamed: 0  level_0  index  \\\n","0  0           2051     104     \n","1  1           2087     140     \n","2  2           2357     410     \n","3  3           2588     641     \n","4  4           3854     1907    \n","5  5           2565     618     \n","6  6           617      617     \n","7  7           918      918     \n","8  8           2118     171     \n","9  9           3170     1223    \n","\n","                                                                                                                                                                 content  \\\n","0  √Ä th·∫ø b·∫°n √Ω h√°t b√†i j jui c√≥ ai gi·ªõi thi·ªáu cho m√¨nh ƒëc ko. L√¢u jui m√¨nh ch·∫≥ng nghe b√†i h√°t m·ªõi n√†o c·∫£                                                                   \n","1  Th·∫ßy ƒëang nghƒ© v·ªÅ c√¥ em xinh t∆∞∆°i n√†y ...                                                                                                                               \n","2  ·ª¶a t·ª© qu√Ω c√≥ ch·∫∑t ƒë√¥i heo ƒëc ƒë√¢u, l·ªô t·ª© qu√Ω r·ªìi nhaaa hahhahaha t cho m th√∫i Tr·∫ßn Th·ªã Linh √Ånh Bui Thi My Ngoc Thanh H√† Thanh Th·∫£o Th√πy Trang L√™ Th·ªã Oanh Ho√†ng Linhc   \n","3  t·ªët, c·ª© gi√° cao ƒëi -_- ch·ª© bh n√¥ng d√¢n c≈©ng mua ƒëc xe th√¨ k bi·∫øt ƒë∆∞·ªùng n√≥ c√≤n ra c√°i g√¨                                                                                 \n","4  M·∫•y c√°i quan b·ªánh t·∫≠t c·ªßa th·ªùi ƒë·∫°i                                                                                                                                      \n","5  M·∫∑t n√≥ l·∫°i ng√°o ü§£                                                                                                                                                       \n","6  Ngu cmm.                                                                                                                                                                \n","7  ai coi tr·∫ßn d·∫ßn v√† ng√¥ k·ª∑ l√† k·∫ª ngu ng√≥c...                                                                                                                             \n","8  Nh∆∞ng m√† khi t·ªè t√¨nh xong r·ªìi b·∫•t b·∫°i v·ª´a m·∫•t ng m√¨nh th∆∞∆°ng v·ª´a m·∫•t lu√¥n b·∫°n üôÇ                                                                                         \n","9  t·ªôc caaaa                                                                                                                                                               \n","\n","                                    index_spans          token  \\\n","0  []                                            []              \n","1  []                                            []              \n","2  []                                            []              \n","3  []                                            []              \n","4  []                                            []              \n","5  []                                            []              \n","6  [0, 1, 2, 3, 4, 5, 6]                         [Ngu cmm]       \n","7  [29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39]  [k·∫ª ngu ng√≥c]   \n","8  []                                            []              \n","9  []                                            []              \n","\n","                                                                                                                     seq  \n","0  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]                                      \n","1  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]                                                                                         \n","2  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]  \n","3  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]                                         \n","4  [0, 0, 0, 0, 0, 0, 0, 0]                                                                                               \n","5  [0, 0, 0, 0, 0]                                                                                                        \n","6  [1, 1, 0]                                                                                                              \n","7  [0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0]                                                                                   \n","8  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]                                                              \n","9  [0, 0]                                                                                                                 "]},"execution_count":9,"metadata":{},"output_type":"execute_result"}],"source":["data['token'], data['seq'] =  span_convert(text_data, spans_data)\n","dev['token'], dev['seq'] =  span_convert(text_dev, spans_dev)\n","test['token'], test['seq'] =  span_convert(text_test, spans_test)\n","data.head(10)\n","train = deepcopy(data)"]},{"cell_type":"markdown","metadata":{"id":"S8PofulBbdK3"},"source":["# EVALUATION METRIC"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Aijjzh9-XluP"},"outputs":[],"source":["import sys\n","import os\n","import os.path\n","from scipy.stats import sem\n","import numpy as np\n","from ast import literal_eval\n","\n","def f1(predictions, gold):\n","    \"\"\"\n","    F1 (a.k.a. DICE) operating on two lists of offsets (e.g., character).\n","    >>> assert f1([0, 1, 4, 5], [0, 1, 6]) == 0.5714285714285714\n","    :param predictions: a list of predicted offsets\n","    :param gold: a list of offsets serving as the ground truth\n","    :return: a score between 0 and 1\n","    \"\"\"\n","    if len(gold) == 0:\n","        return 1. if len(predictions) == 0 else 0.\n","    if len(predictions) == 0:\n","        return 0.\n","    predictions_set = set(predictions)\n","    gold_set = set(gold)\n","    nom = 2 * len(predictions_set.intersection(gold_set))\n","    denom = len(predictions_set) + len(gold_set)\n","    return float(nom)/float(denom)\n","\n","\n","def evaluate(pred, gold):\n","    \"\"\"\n","    Based on https://github.com/felipebravom/EmoInt/blob/master/codalab/scoring_program/evaluation.py\n","    :param pred: file with predictions\n","    :param gold: file with ground truth\n","    :return:\n","    \"\"\"\n","    # # read the predictions\n","    # pred_lines = pred.readlines()\n","    # # read the ground truth\n","    # gold_lines = gold.readlines()\n","\n","    pred_lines = pred\n","    gold_lines = gold\n","\n","    # only when the same number of lines exists\n","    if (len(pred_lines) == len(gold_lines)):\n","        data_dic = {}\n","        for n, line in enumerate(gold_lines):\n","            parts = line.split('\\t')\n","            if len(parts) == 2:\n","                data_dic[int(parts[0])] = [literal_eval(parts[1])]\n","            else:\n","                raise ValueError('Format problem for gold line %d.', n)\n","\n","        for n, line in enumerate(pred_lines):\n","            parts = line.split('\\t')\n","            if len(parts) == 2:\n","                if int(parts[0]) in data_dic:\n","                    try:\n","                        data_dic[int(parts[0])].append(literal_eval(parts[1]))\n","                    except ValueError:\n","                        # Invalid predictions are replaced by a default value\n","                        data_dic[int(parts[0])].append([])\n","                else:\n","                    raise ValueError('Invalid text id for pred line %d.', n)\n","            else:\n","                raise ValueError('Format problem for pred line %d.', n)\n","\n","        # lists storing gold and prediction scores\n","        scores = []\n","        for id in data_dic:\n","            if len(data_dic[id]) == 2:\n","                gold_spans = data_dic[id][0]\n","                pred_spans = data_dic[id][1]\n","                scores.append(f1(pred_spans, gold_spans))\n","            else:\n","                sys.exit('Repeated id in test data.')\n","\n","        return (np.mean(scores), sem(scores))\n"]},{"cell_type":"markdown","metadata":{"id":"Uo9Rs5A3zAa8"},"source":["# Word Embedding"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":45651,"status":"ok","timestamp":1641748440177,"user":{"displayName":"Nap Lan Dau","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgyLJ7xblBnvLEyFishN4Q4xZSVXTxeaLzYRJU=s64","userId":"01595996840706054717"},"user_tz":-420},"id":"kXeR0uQGQaxF","outputId":"a661a44a-8941-4fdf-b02a-90e9d0f6907a"},"outputs":[{"name":"stdout","output_type":"stream","text":["GloVe data loaded\n"]}],"source":["# Read embedding\n","word_dict = []\n","embeddings_index = {}\n","f = open('word2vec_vi_words_100dims.txt')\n","for line in f:\n","    values = line.split(' ')\n","    word = values[0] \n","    word_dict.append(word)\n","    coefs = np.asarray(values[1:], dtype='float32')\n","    embeddings_index[word] = coefs\n","f.close()\n","\n","print('GloVe data loaded')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"g09huNrtzD41"},"outputs":[],"source":["words = word_dict\n","num_words = len(words)\n","\n","# Dictionary word:index pair\n","# word is key and its value is corresponding index\n","word_to_index = {w : i + 2 for i, w in enumerate(words)}\n","word_to_index[\"UNK\"] = 1\n","word_to_index[\"PAD\"] = 0\n","\n","# Dictionary lable:index pair\n","idx2word = {i: w for w, i in word_to_index.items()}"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"d3qi1TZZzHMp"},"outputs":[],"source":["# first create a matrix of zeros, this is our embedding matrix\n","embedding_dim = 100\n","max_len = 10\n","max_feature = 1000\n","embedding_matrix = np.zeros((num_words, embedding_dim))\n","\n","# for each word in out tokenizer lets try to find that work in our w2v model\n","for word, i in word_to_index.items():\n","    if i > max_feature:\n","        continue\n","    embedding_vector = embeddings_index.get(word)\n","    if embedding_vector is not None:\n","        # we found the word - add that words vector to the matrix\n","        embedding_matrix[i] = embedding_vector\n","    else:\n","        # doesn't exist, assign a random vector\n","        embedding_matrix[i] = np.random.randn(embedding_dim)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"oxUF87qWzImk"},"outputs":[],"source":[" # mapping for token cases\n","case2Idx = {'1': 1, '0': 0}\n","caseEmbeddings = np.identity(len(case2Idx), dtype='float32')  # identity matrix used \n","\n","char2Idx = {\"PADDING\": 0, \"UNKNOWN\": 1}\n","for c in \" 0123456789abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ.,-_()[]{}!?:;#'\\\"/\\\\%$`&=*+@^~|<>\":\n","    char2Idx[c] = len(char2Idx)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EPg9dMti68ey"},"outputs":[],"source":["from sklearn.model_selection import train_test_split\n","\n","y = data['seq']\n","X = data['content']\n","\n","y_test = test['seq']\n","X_test = test['content']\n","\n","y_dev = dev['seq']\n","X_dev = dev['content']\n","\n","X_train = X\n","y_train = y"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DtgToomq_PGl"},"outputs":[],"source":["tknzr2 = TweetTokenizer()\n","\n","def custom_tokenizer(text_data):\n","    text_data = text_data.lower()\n","    return tknzr2.tokenize(text_data)\n","\n","def encoding(X, y, isTest = True):\n","    sentences = []\n","    \n","    for t in X:\n","        sentences.append(custom_tokenizer(t))\n","\n","    X = []\n","    for s in sentences:\n","        sent = []\n","        for w in s:\n","            try:\n","                w = w.lower()\n","                sent.append(word_to_index[w])\n","            except:\n","                sent.append(word_to_index[\"UNK\"])\n","        X.append(sent)\n","           \n","    X = pad_sequences(maxlen = max_len, sequences = X, padding = \"post\", value = word_to_index[\"PAD\"])\n","\n","    if isTest:\n","        y = pad_sequences(maxlen=max_len, sequences=y, padding=\"post\", value=word_to_index[\"PAD\"])\n","        y = to_categorical(y, num_classes=2)\n","    else:\n","        y = None\n","\n","    return (X,y)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vCl6EvPjAavV"},"outputs":[],"source":["X1, y1 = encoding(X_train, y_train)\n","X2, y2 = encoding(X_dev, y_dev)\n","X3, y3 = encoding(X_test, y_test)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":41485,"status":"ok","timestamp":1641748483897,"user":{"displayName":"Nap Lan Dau","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgyLJ7xblBnvLEyFishN4Q4xZSVXTxeaLzYRJU=s64","userId":"01595996840706054717"},"user_tz":-420},"id":"Cjnn2tguAdY8","outputId":"09d851ad-7581-4178-c953-dee5c69443ec"},"outputs":[{"name":"stdout","output_type":"stream","text":["WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/keras/backend/tensorflow_backend.py:133: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n","WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow_core/python/ops/math_ops.py:2509: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use tf.where in 2.0, which has the same broadcast rule as np.where\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","input_1 (InputLayer)         (None, 10)                0         \n","_________________________________________________________________\n","embedding_1 (Embedding)      (None, 10, 100)           158751000 \n","_________________________________________________________________\n","dropout_1 (Dropout)          (None, 10, 100)           0         \n","_________________________________________________________________\n","bidirectional_1 (Bidirection (None, 10, 20)            8880      \n","_________________________________________________________________\n","time_distributed_1 (TimeDist (None, 10, 10)            210       \n","_________________________________________________________________\n","crf_1 (CRF)                  (None, 10, 2)             30        \n","=================================================================\n","Total params: 158,760,120\n","Trainable params: 158,760,120\n","Non-trainable params: 0\n","_________________________________________________________________\n"]}],"source":["# BiLSTM - CRF \n","from keras.layers import LSTM, Dense, TimeDistributed, Embedding, Bidirectional, Flatten, Dropout\n","\n","from keras.models import Model, Input\n","from keras_contrib.layers import CRF\n","from keras.utils.vis_utils import plot_model\n","\n","import warnings\n","warnings.filterwarnings(\"ignore\")\n","\n","# from keras.metrics import BinaryAccuracy, Precision, Recall, AUC\n","\n","input = Input(shape = (max_len,))\n","model = Embedding(input_dim=num_words+2,\n","                    output_dim=embedding_dim,\n","                    embeddings_initializer=Constant(embedding_matrix),\n","                    input_length=max_len,\n","                    trainable=True)(input)\n","\n","model = Dropout(0.1)(model)\n","model = Bidirectional(LSTM(units = max_len, return_sequences=True, recurrent_dropout=0.1))(model)\n","model = TimeDistributed(Dense(max_len, activation=\"relu\"))(model)\n","crf = CRF(2)  \n","out = crf(model)  # output\n","\n","model = Model(input, out)\n","model.compile(optimizer=\"adam\", loss=crf.loss_function, metrics=['accuracy'])\n","\n","model.summary()\n","\n","plot_model(model,to_file=\"bilstm-crf.pdf\",show_shapes=True,show_layer_names=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":252395,"status":"ok","timestamp":1641749165477,"user":{"displayName":"Nap Lan Dau","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgyLJ7xblBnvLEyFishN4Q4xZSVXTxeaLzYRJU=s64","userId":"01595996840706054717"},"user_tz":-420},"id":"EInItV9iESG_","outputId":"4e916520-74be-4987-a6ac-7a5fd7183dd7"},"outputs":[{"name":"stdout","output_type":"stream","text":["Train on 3894 samples, validate on 488 samples\n","Epoch 1/4\n","3894/3894 [==============================] - 1s 177us/step - loss: 0.5223 - acc: 0.8888 - val_loss: 0.3817 - val_acc: 0.5721\n","Epoch 2/4\n","3894/3894 [==============================] - 1s 181us/step - loss: 0.3129 - acc: 0.8888 - val_loss: 0.2514 - val_acc: 0.8525\n","Epoch 3/4\n","3894/3894 [==============================] - 1s 172us/step - loss: 0.2206 - acc: 0.8888 - val_loss: 0.2151 - val_acc: 0.8857\n","Epoch 4/4\n","3894/3894 [==============================] - 1s 176us/step - loss: 0.1961 - acc: 0.8888 - val_loss: 0.2073 - val_acc: 0.8850\n"]},{"data":{"text/plain":["<keras.callbacks.History at 0x7f73743f6310>"]},"execution_count":29,"metadata":{},"output_type":"execute_result"}],"source":["from keras.callbacks import ModelCheckpoint\n","import warnings\n","warnings.filterwarnings(\"ignore\")\n","\n","checkpointer = ModelCheckpoint(filepath = 'model_detection_19_word.h5',\n","                       verbose = 0,\n","                       mode = 'auto',\n","                       save_best_only = True,\n","                       monitor='val_loss')\n","\n","model.fit(X1, np.array(y1), batch_size=512, epochs=4, validation_data=(X2, y2), callbacks=[checkpointer])\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3yK1M8yrEMEu"},"outputs":[],"source":["y_pred = model.predict(X3)\n","y_pred = np.argmax(y_pred, axis=-1)\n","y_test_true = np.argmax(y3, -1)\n","# y_pred = [[i for i in row] for row in y_pred]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PpFtuOjBCLOJ"},"outputs":[],"source":["raw_y = decoding(X_test, X3, y_pred)\n"]},{"cell_type":"markdown","metadata":{"id":"crS4RoxuQBfa"},"source":["error df"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ycS2-kuKMuoQ"},"outputs":[],"source":["token_predict, seq_predict = span_convert(text_test, raw_y)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3yBjPKWFQAvR"},"outputs":[],"source":["error = pd.DataFrame()\n","error['True'] = test['token']\n","error['Pred'] = token_predict"]},{"cell_type":"markdown","metadata":{"id":"0lnQHCAIzzqb"},"source":["# PREDICT"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SRQ3jz39zy_m"},"outputs":[],"source":["def tokenize(text, pos):\n","    tokens = text.split()\n","    alignment = []\n","    start = 0\n","    for t in tokens:\n","        res = text.find(t, start)\n","        alignment.append(pos[res:res + len(t)])\n","        start = res + len(t)\n","    assert len(tokens) == len(alignment)\n","    return tokens, alignment\n","def y_true(data):\n","  index_true = []\n","  data['index_spans'] = data['index_spans'].apply(literal_eval)\n","  for i in range(len(data)):\n","    text = data['content'][i]\n","    pos = [i for i in range(len(text))]\n","    df_point = pd.DataFrame()\n","    df_point['spans'] = pos\n","    df_point['spans'] = 0\n","    if not data['index_spans'][i]:\n","      index_true.append(list(df_point['spans']))\n","    else:\n","      for j in data['index_spans'][i]:\n","        df_point['spans'][j] = 1\n","      index_true.append(list(df_point['spans']))\n","  return index_true\n","def y_pred(test, error):\n","  index_pred = []\n","  for dt in range(len(error)):\n","    value_predict_i = error['Pred'][dt]\n","    text = test['content'][dt]\n","    pos = [i for i in range(len(text))]\n","    tokens, alignment = tokenize(text, pos)\n","    df_point = pd.DataFrame()\n","    df_point['spans'] = pos\n","    df_point['spans'] = 0\n","    for j in range(len(tokens)):\n","      if tokens[j] in value_predict_i:\n","        for ali in alignment[j]:\n","            df_point['spans'][ali] = 1\n","    index_pred.append(list(df_point['spans']))\n","  return index_pred\n","true = y_true(test)\n","pred = y_pred(test, error)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7h-ng6cx8pay"},"outputs":[],"source":["from sklearn.metrics import precision_recall_fscore_support\n","scores_f1_macro = []\n","scores_f1_micro = []\n","scores_precision_macro = []\n","scores_precision_micro = []\n","scores_recall_macro = []\n","scores_recall_micro = []\n","\n","for i in range(len(true)):\n","  score_macro = precision_recall_fscore_support(true[i], pred[i], average='macro')\n","  score_micro = precision_recall_fscore_support(true[i], pred[i], average='micro')\n","\n","  scores_f1_macro.append(score_macro[2])\n","  scores_f1_micro.append(score_micro[2])\n","  scores_precision_macro.append(score_macro[0])\n","  scores_precision_micro.append(score_micro[0])\n","  scores_recall_macro.append(score_macro[1])\n","  scores_recall_micro.append(score_micro[1])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CHfFUCwz8wbY"},"outputs":[],"source":["scores = pd.DataFrame()\n","scores['F1-micro'] = [np.mean(scores_f1_micro)]\n","scores['F1-macro'] = [np.mean(scores_f1_macro)]\n","scores['Precision-macro'] = [np.mean(scores_precision_macro)]\n","scores['Precision-micro'] = [np.mean(scores_precision_micro)]\n","scores['Recall-macro'] = [np.mean(scores_recall_macro)]\n","scores['Recall-micro'] = [np.mean(scores_recall_micro)]"]},{"cell_type":"markdown","metadata":{"id":"f6tVYPVA80M2"},"source":["# Save"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0vwn1rza9bB3"},"outputs":[],"source":["scores.to_csv('score.csv')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"b64ikatF9eFg"},"outputs":[],"source":["error.to_csv('error.csv')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"b5Vq4lBD_np7"},"outputs":[],"source":["model.save('model.h5')\n"]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":[],"machine_shape":"hm","provenance":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.10.6 (main, Nov 14 2022, 16:10:14) [GCC 11.3.0]"},"vscode":{"interpreter":{"hash":"916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"}}},"nbformat":4,"nbformat_minor":0}
