{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"_KFIOHagGKbc"},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","from ast import literal_eval\n","import re\n","from ast import literal_eval"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4358,"status":"ok","timestamp":1641556649092,"user":{"displayName":"Nap Lan Dau","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgyLJ7xblBnvLEyFishN4Q4xZSVXTxeaLzYRJU=s64","userId":"01595996840706054717"},"user_tz":-420},"id":"AsYLPNS2I9P-","outputId":"76891995-d147-4646-aebf-b245ee73334e"},"outputs":[],"source":["# Word segmenter\n","!pip3 install vncorenlp\n","\n","# Download VnCoreNLP-1.1.1.jar & its word segmentation component (i.e. RDRSegmenter) \n","!mkdir -p vncorenlp/models/wordsegmenter\n","!wget https://raw.githubusercontent.com/vncorenlp/VnCoreNLP/master/VnCoreNLP-1.1.1.jar\n","!wget https://raw.githubusercontent.com/vncorenlp/VnCoreNLP/master/models/wordsegmenter/vi-vocab\n","!wget https://raw.githubusercontent.com/vncorenlp/VnCoreNLP/master/models/wordsegmenter/wordsegmenter.rdr\n","!mv VnCoreNLP-1.1.1.jar vncorenlp/ \n","!mv vi-vocab vncorenlp/models/wordsegmenter/\n","!mv wordsegmenter.rdr vncorenlp/models/wordsegmenter/"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":24,"status":"ok","timestamp":1641556649094,"user":{"displayName":"Nap Lan Dau","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgyLJ7xblBnvLEyFishN4Q4xZSVXTxeaLzYRJU=s64","userId":"01595996840706054717"},"user_tz":-420},"id":"BSYQ85ybG7Hg","outputId":"e65ce208-1ccf-41ef-8e0b-d8b2a588b3c2"},"outputs":[],"source":["from IPython.core.interactiveshell import InteractiveShell\n","InteractiveShell.ast_node_interactivity = \"all\"\n","\n","\n","pd.set_option('display.max_rows', None)\n","pd.set_option('display.max_columns', None)\n","pd.set_option('display.width', None)\n","pd.set_option('display.max_colwidth', -1)\n","pd.options.display.max_rows"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":191},"executionInfo":{"elapsed":341,"status":"ok","timestamp":1641570592726,"user":{"displayName":"Nap Lan Dau","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgyLJ7xblBnvLEyFishN4Q4xZSVXTxeaLzYRJU=s64","userId":"01595996840706054717"},"user_tz":-420},"id":"fn-nOSPGHMGw","outputId":"e51b26de-f1e2-43c7-dce2-05f35f416294"},"outputs":[],"source":["train = pd.read_csv('train.csv')\n","dev = pd.read_csv('dev.csv')\n","test = pd.read_csv('test.csv')\n","test['index_spans'] = test['index_spans'].apply(literal_eval)\n","train['index_spans'] = train['index_spans'].apply(literal_eval)\n","dev['index_spans'] = dev['index_spans'].apply(literal_eval)\n","\n","headers = ['Unnamed: 0',  'content', 'index_spans', 'Span']\n","train.columns = headers\n","dev.columns = headers\n","test.columns = headers\n","test.head(2)"]},{"cell_type":"markdown","metadata":{"id":"7t9vZ6idK66i"},"source":["# Pre-processing"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3916,"status":"ok","timestamp":1641570600274,"user":{"displayName":"Nap Lan Dau","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgyLJ7xblBnvLEyFishN4Q4xZSVXTxeaLzYRJU=s64","userId":"01595996840706054717"},"user_tz":-420},"id":"yuC4Y4GKK8zf","outputId":"341e3eb4-0a3e-42df-f686-25ab6f8f04ba"},"outputs":[],"source":["def unicode(data):\n","  for i in range(len(data)):\n","    data['content'][i] = data['content'][i].replace(\"òa\", \"oà\")\n","    data['content'][i] = data['content'][i].replace(\"óa\", \"oá\")\n","    data['content'][i] = data['content'][i].replace(\"ỏa\", \"oả\")\n","    data['content'][i] = data['content'][i].replace(\"õa\", \"oã\")\n","    data['content'][i] = data['content'][i].replace(\"ọa\", \"oạ\")\n","    data['content'][i] = data['content'][i].replace(\"òe\", \"oè\")\n","    data['content'][i] = data['content'][i].replace(\"óe\", \"oé\")\n","    data['content'][i] = data['content'][i].replace(\"ỏe\", \"oẻ\")\n","    data['content'][i] = data['content'][i].replace(\"õe\", \"oẽ\")\n","    data['content'][i] = data['content'][i].replace(\"ọe\", \"oẹ\")\n","    data['content'][i] = data['content'][i].replace(\"ùy\", \"uỳ\")\n","    data['content'][i] = data['content'][i].replace(\"úy\", \"uý\")\n","    data['content'][i] = data['content'][i].replace(\"ủy\", \"uỷ\")\n","    data['content'][i] = data['content'][i].replace(\"ũy\", \"uỹ\")\n","    data['content'][i] = data['content'][i].replace(\"ụy\", \"uỵ\")\n","    data['content'][i] = data['content'][i].replace(\"Ủy\", \"Uỷ\")\n","  return data\n","def replace(text, pattern, replacement, pos):\n","    matches = [0]\n","\n","    def capture_and_replace(match, ret):\n","        matches.extend([match.start() + 1, match.end()])\n","        return ret\n","\n","    l = len(text)\n","    text = re.sub(pattern, lambda match: capture_and_replace(match, replacement), text, flags=re.IGNORECASE)\n","    matches.append(l)\n","    slices = np.array_split(matches, int(len(matches) / 2))\n","    res = []\n","    for s in slices:\n","        res += pos[s[0]:s[1]]\n","    assert len(text) == len(res)\n","    return text, res\n","def preprocess(text, pos):\n","    \n","    # collapse duplicated punctuations \n","    punc = ',. !?\\\"\\''\n","    for c in punc:\n","        pat = '([' + c + ']{2,})'\n","        text, pos = replace(text, pat, c, pos)\n","    assert len(text) == len(pos)\n","    return text, pos\n","\n","test = unicode(test)\n","train = unicode(train)\n","dev = unicode(dev)"]},{"cell_type":"markdown","metadata":{"id":"VvaXLDO6PIcA"},"source":["# Tokeniner"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZqnC7MwaqV7_"},"outputs":[],"source":["df_test = pd.DataFrame()\n","df_test['spans'] = test['index_spans']\n","df_test['text'] = test['content']\n","df_test.to_csv('df_test.csv')\n","\n","df_train = pd.DataFrame()\n","df_train['spans'] = train['index_spans']\n","df_train['text'] = train['content']\n","df_train.to_csv('df_train.csv')\n","\n","df_dev = pd.DataFrame()\n","df_dev['spans'] = dev['index_spans']\n","df_dev['text'] = dev['content']\n","df_dev.to_csv('df_dev.csv')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pnL6rnlKHsik"},"outputs":[],"source":["from vncorenlp import VnCoreNLP\n","annotator = VnCoreNLP(\"/content/vncorenlp/VnCoreNLP-1.1.1.jar\", annotators=\"wseg\", max_heap_size='-Xmx500m')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":21,"status":"ok","timestamp":1641180328015,"user":{"displayName":"Lưu Đức Cảnh","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"15467637707317635561"},"user_tz":-420},"id":"DtPB0PVULquy","outputId":"899752fe-72fb-41b1-9474-9a71e689d496"},"outputs":[],"source":["text = test['content'][325]\n","text = 'Nham quá'\n","annotator_text = annotator.tokenize(text)\n","tokens = []\n","for i in range(len(annotator_text)):\n","  for j in range(len(annotator_text[i])):\n","    tokens.append(annotator_text[i][j])\n","tokens"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OVSlJYNRMvuf"},"outputs":[],"source":["import more_itertools as mit\n","from itertools import groupby\n","from operator import itemgetter\n","def find_ranges(span):\n","  ranges =[]\n","  for k,g in groupby(enumerate(span),lambda x:x[0]-x[1]):\n","      group = (map(itemgetter(1),g))\n","      group = list(map(int,group))\n","      ranges.append((group[0],group[-1]))\n","  return ranges\n","def tokenize_word(text, pos):\n","    annotator_text = annotator.tokenize(text)\n","    tokens = []\n","    for i in range(len(annotator_text)):\n","      for j in range(len(annotator_text[i])):\n","        tokens.append(annotator_text[i][j])\n","\n","    alignment = []\n","    start = 0\n","    for t in tokens:\n","      if t == \"_\":    \n","        res = text.find(t, start)\n","        alignment.append(pos[res:res + len(t)])\n","        start = res + len(t)\n","      elif t.find(\"_.\", 0) != -1 or t.find(\"_-\", 0) != -1:       # k xuất hiện _.\n","        t = t.replace(\"_\",\"\")\n","        res = text.find(t, start)\n","        alignment.append(pos[res:res + len(t)])\n","        start = res + len(t)\n","      else:\n","        t = t.lstrip(\"_\")\n","        t = t.replace(\"_\", \" \")\n","        res = text.find(t, start)\n","        alignment.append(pos[res:res + len(t)])\n","        start = res + len(t)       \n","    assert len(tokens) == len(alignment)\n","    return tokens, alignment\n","def annotate(spans, alignment, tokens):\n","  Tag = []\n","  annotations = pd.DataFrame()\n","  annotations['Tokens'] = tokens\n","  for i in range(len(tokens)):\n","    Tag.append(\"O\")\n","  annotations['Tag'] = Tag\n","  for span in spans:\n","      i = 0\n","      while i < len(alignment):\n","          if alignment[i][-1] < span[0]:\n","            i += 1\n","          elif alignment[i][0] <= span[0] <= alignment[i][-1]:\n","              annotations['Tag'][i] = ('B-T')\n","              i += 1\n","          elif span[0] < alignment[i][0] <= span[-1]:\n","              annotations['Tag'][i] = ('I-T')\n","              i += 1\n","          elif alignment[i][0] > span[-1]:\n","              break\n","  return annotations['Tag']\n","def load_data(path):\n","  tsd = pd.read_csv(path)\n","  tsd.spans = tsd.spans.apply(literal_eval)\n","  data = []\n","  for row in tsd.iterrows():\n","      span = row[1]['spans']\n","      text = row[1]['text']\n","      temp = []\n","      text_spans = []\n","      if span:\n","        segments = list(find_ranges(span))\n","        for seg in segments:\n","          if ((len(seg) == 2) and seg[0] == seg[1]):\n","            temp.append([seg[0]])\n","            text_spans.append(text[seg[0]: seg[-1] + 1])\n","          else:\n","            temp.append([seg[0], seg[-1]])\n","            text_spans.append(text[seg[0]: seg[-1] + 1])\n","      data.append({'text': text, 'spans': temp, 'text_spans': text_spans}) \n","  return data"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Fia3UwtPpNxc"},"outputs":[],"source":["test_data = load_data('df_test.csv')\n","train_data = load_data('df_train.csv')\n","dev_data = load_data('df_dev.csv')\n"]},{"cell_type":"markdown","metadata":{"id":"GraLvJq-sodS"},"source":["# Annotate BIO"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wpXlKvDxvjoX"},"outputs":[],"source":["def annotate(spans, alignment, tokens):\n","  Tag = []\n","  annotations = pd.DataFrame()\n","  annotations['Tokens'] = tokens\n","  for i in range(len(tokens)):\n","    Tag.append(\"O\")\n","  annotations['Tag'] = Tag\n","  for span in spans:\n","      i = 0\n","      while i < len(alignment):\n","          if alignment[i][-1] < span[0]:\n","            i += 1\n","          elif alignment[i][0] <= span[0] <= alignment[i][-1]:\n","              annotations['Tag'][i] = ('B-T')\n","              i += 1\n","          elif span[0] < alignment[i][0] <= span[-1]:\n","              annotations['Tag'][i] = ('I-T')\n","              i += 1\n","          elif alignment[i][0] > span[-1]:\n","              break\n","  return annotations['Tag']"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IakT6XgjqMnc"},"outputs":[],"source":["def data_BIO(data):\n","  formated_data = []\n","  for d in data:\n","    text = d['text']\n","    pos = [i for i in range(len(text))]\n","    text, pos = preprocess(text, pos)\n","    tokens, alignment = tokenize_word(text, pos)\n","    annotations = annotate(d['spans'], alignment, tokens)\n","    ls = [[tokens[i], annotations[i]] for i in range(len(tokens))]\n","    \n","    formated_data.extend(ls)\n","    formated_data.append([None])\n","  df_final =  pd.DataFrame(formated_data, columns= ['Word', 'Tag'])\n","  sentence_id = []\n","  sentence = 0\n","  for i in range(len(df_final)):\n","    if(df_final['Word'][i] != None):\n","      sentence_id.append(sentence)\n","    else:\n","      sentence_id.append(np.nan)\n","      sentence += 1\n","  df_final['sentence_id'] = sentence_id\n","  df_final.dropna(inplace=True)\n","  df_final['sentence_id'] = df_final['sentence_id'].astype(\"int64\")\n","  return df_final\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"elapsed":21204,"status":"ok","timestamp":1641570640444,"user":{"displayName":"Nap Lan Dau","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgyLJ7xblBnvLEyFishN4Q4xZSVXTxeaLzYRJU=s64","userId":"01595996840706054717"},"user_tz":-420},"id":"aHoIEsFIrAt_","outputId":"42c7b9f6-57ce-4868-8b65-362a17d88da5"},"outputs":[],"source":["test_IBO = data_BIO(test_data)\n","train_IBO = data_BIO(train_data)\n","dev_IBO = data_BIO(dev_data)\n","test_IBO"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_DJC2DoD6z9Y"},"outputs":[],"source":["test_IBO.reset_index(inplace=True)\n","dev_IBO.reset_index(inplace=True)\n","train_IBO.reset_index(inplace=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sYxSQkjOvPGe"},"outputs":[],"source":["train_IBO.to_csv('train_BIO.csv')\n","dev_IBO.to_csv('dev_BIO.csv')\n","test_IBO.to_csv('test_BIO.csv')"]}],"metadata":{"colab":{"collapsed_sections":[],"provenance":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.10.6 (main, Nov 14 2022, 16:10:14) [GCC 11.3.0]"},"vscode":{"interpreter":{"hash":"916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"}}},"nbformat":4,"nbformat_minor":0}
